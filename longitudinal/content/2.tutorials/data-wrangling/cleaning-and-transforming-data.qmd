---
title: "Cleaning and Transforming Data for Longitudinal Analysis"
---

### Introduction

Cleaning and transforming data is one of the most critical steps in the longitudinal data analysis pipeline. Longitudinal datasets are complex, often containing repeated measures, missing data, outliers, and other irregularities that need to be handled carefully to avoid biased or misleading results. Proper data cleaning and transformation ensure that your dataset is ready for analysis and that you can make meaningful inferences.

In this tutorial, we will go through a systematic approach to cleaning and transforming longitudinal data, with detailed explanations and examples. This tutorial is designed for students at various stages, from undergraduate to postdoctoral researchers, and assumes some basic familiarity with R programming and data manipulation. 

### Goals of Data Cleaning and Transformation

1. **Ensure Data Integrity**: Detect and fix irregularities, such as missing values, inconsistencies, or coding errors.
2. **Prepare Data for Longitudinal Models**: Restructure data into the correct format for analysis, especially when working with wide and long formats.
3. **Maximize Valid Data**: Address issues related to missing data, outliers, and erroneous values.
4. **Optimize Data for Analysis**: Ensure that the data is organized, standardized, and ready for modeling, making it easier to interpret the results.

### 1. Handling Missing Data in Longitudinal Datasets

Missing data is one of the most common issues in longitudinal studies. It is critical to understand the pattern of missingness in your data, as this can influence the choice of techniques to handle the missing data. The three main types of missing data patterns are:

- **MCAR (Missing Completely at Random)**: The probability of missing data is unrelated to the study variables.
- **MAR (Missing at Random)**: The probability of missing data is related to observed data, but not to the unobserved data.
- **MNAR (Missing Not at Random)**: The probability of missing data is related to the unobserved data.

Depending on the pattern, there are various strategies to handle missing data:

- **Listwise Deletion**: Remove any rows with missing values. This approach is simple but may reduce the sample size, leading to biased results if the missingness is not random.
  
    ```r
    # Remove rows with any missing values
    cleaned_data <- na.omit(your_data)
    ```

- **Mean/Median Imputation**: Impute missing values by replacing them with the mean or median of the observed data. This method works when the missingness is random but can underestimate variability.

    ```r
    # Mean imputation for a single variable
    library(dplyr)
    your_data <- your_data %>%
      mutate(variable = ifelse(is.na(variable), mean(variable, na.rm = TRUE), variable))
    ```

- **Multiple Imputation**: A more sophisticated approach where multiple datasets are generated, each with imputed values. These datasets are analyzed separately, and results are pooled. This method is recommended when missing data is MAR.

    ```r
    # Multiple imputation using mice package
    library(mice)
    imputed_data <- mice(your_data, m = 5, method = 'pmm', seed = 123)
    completed_data <- complete(imputed_data)
    ```

### 2. Reshaping Data: From Wide to Long Format

Longitudinal data is often collected in a "wide" format, where each row represents a subject, and each column represents a time point or measurement. However, most longitudinal models, such as mixed-effects models, require the data to be in "long" format, where each row represents a single time point for each subject.

- **Wide Format**: Each subject is in one row, and each time point is a separate column.
- **Long Format**: Each subject has multiple rows, one for each time point.

To reshape data from wide to long format, use the `pivot_longer()` function in `tidyr`.

```r
# Example of reshaping from wide to long format
library(tidyr)

wide_data <- data.frame(
  id = 1:3,
  time1 = c(5, 3, 6),
  time2 = c(7, 6, 8),
  time3 = c(8, 7, 9)
)

long_data <- pivot_longer(wide_data, cols = starts_with("time"), names_to = "time", values_to = "measure")
```

In some cases, you may need to go from long to wide format (e.g., for summary purposes), which can be done using `pivot_wider()`:

```r
# Reshaping from long to wide format
wide_data <- pivot_wider(long_data, names_from = time, values_from = measure)
```

### 3. Detecting and Handling Outliers

Outliers can have a significant impact on longitudinal models, especially if they represent erroneous data. It is important to detect and carefully consider how to deal with outliers.

Common methods for detecting outliers include:
- **Z-Scores**: A common method for detecting outliers is to calculate z-scores for each observation and flag observations with z-scores greater than 3 (or less than -3).

```r
# Detect outliers using Z-scores
your_data <- your_data %>%
  mutate(z_score = (variable - mean(variable, na.rm = TRUE)) / sd(variable, na.rm = TRUE)) %>%
  filter(abs(z_score) < 3)  # Remove outliers
```

- **Visual Inspection**: Visualizing data using boxplots or scatterplots is often useful for identifying potential outliers.

```r
# Create a boxplot to visualize potential outliers
library(ggplot2)
ggplot(your_data, aes(x = time, y = measure)) + 
  geom_boxplot() + 
  theme_minimal()
```

Outliers may either be legitimate data points that represent extreme but real values, or they may be data entry errors. It is important to investigate outliers individually before deciding whether to exclude them.

### 4. Standardizing and Normalizing Variables

Standardization and normalization are common preprocessing steps, especially when using models that rely on assumptions about the distribution of the data.

- **Standardization (Z-scores)**: This process transforms variables so that they have a mean of 0 and a standard deviation of 1. This is especially important when comparing variables that are on different scales.

    ```r
    # Standardizing a variable
    your_data <- your_data %>%
      mutate(variable_scaled = scale(variable))
    ```

- **Normalization**: Normalization rescales data to a range, typically between 0 and 1. This is useful when you want to ensure that all variables are on the same scale.

    ```r
    # Normalizing a variable
    your_data <- your_data %>%
      mutate(variable_normalized = (variable - min(variable)) / (max(variable) - min(variable)))
    ```

### 5. Managing Time-Varying Covariates

In longitudinal studies, it's common to have covariates that change over time, such as medication use or environmental exposure. These are known as **time-varying covariates**. Properly handling these covariates is important for correctly interpreting longitudinal models.

```r
# Example of adding a time-varying covariate
your_data$medication <- ifelse(your_data$time > 2, 1, 0)  # Medication introduced at time point 3
```

### Conclusion

Properly cleaning and transforming longitudinal data is essential for ensuring robust, meaningful analyses. This process involves addressing missing data, restructuring the dataset, detecting outliers, and standardizing variables, among other steps. By taking these steps, you ensure that your data is well-prepared for more advanced modeling techniques.

Longitudinal data poses unique challenges, but with careful cleaning and transformation, you can make the most out of your dataset and derive meaningful insights.
