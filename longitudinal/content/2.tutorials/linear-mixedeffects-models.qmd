---
title: "Linear Mixed-Effects Models"
author: "Your Name"
format: 
  html:
    code-tools: true
    toc: true
    toc-depth: 3
    code-link: true
    df-print: paged
---


### Overview

Linear mixed models (LMMs) are a versatile statistical tool used to analyze data with both fixed effects, which represent population-wide trends, and random effects, which account for individual or group-level variability. They are particularly valuable for hierarchically structured data or repeated measures, such as students within schools or patients monitored over time, where correlations between observations need to be modeled. In longitudinal analyses, LMMs are often preferred because they allow for individual-specific baselines and rates of change by incorporating random intercepts and slopes. This flexibility makes LMMs ideal for capturing both overall trends and individual variations, offering a more nuanced understanding of the data compared to simpler regression models.

<div class="when-to-use-section">

### ðŸ’¡ When to use Linear Mixed Models (LMMs)?

You should consider using LMMs in the following scenarios:

1. You want to understand both overall population trends and individual variations over time.
2. Your outcome variable is measured repeatedly for the same subjects across multiple time points.
3. You have data with a hierarchical or nested data and need to account for correlated observations.
</div>

<div class="getting-started-section">

### ðŸš€ Getting Started with Longitudinal Linear Mixed Models

In this tutorial, we will introduce the concept of linear mixed-effects models (LMMs) and guide you through a simple example using a small dataset. By the end of this tutorial, you will be able to:

1. Understand the basic structure of linear mixed models.
2. Fit a longitudinal linear mixed model using example data in R.
3. Interpret the results of the LMM analysis.

Let's dive in!
</div>

<div class="highlighted-content-header">

### Basic Example 

</div>

For this tutorial, we will use a simple dataset containing the following information for a group of individuals:

- Subject ID
- Time (in years)
- Outcome variable (e.g., a measure of cognitive performance)

```r
# Generate a simple dataset
set.seed(123)
data <- data.frame(
  SubjectID = rep(1:5, each = 4),
  Time = rep(0:3, times = 5),
  Outcome = c(100, 105, 110, 115, 95, 100, 105, 110, 90, 95, 100, 105, 85, 90, 95, 100, 80, 85, 90, 95)
)

# View the first few rows
head(data)
```

#### Fitting Linear Mixed-Effects Models in R

We'll start by using the `lme4` package in R to fit a **random intercept model**, where each individual has their own baseline (intercept).

```r

# Load necessary libraries
library(lme4)

# Fit a random intercept model
model1 <- lmer(Outcome ~ Time + (1 | SubjectID), data = data)

# Display model summary to examine fixed and random effects
summary(model1)

```

#### Model Interpretation

In the output, the fixed effects provide the population-wide average relationship between the predictors (e.g., age) and the response variable (e.g., height). The random effects show how much individual subjects deviate from this average.

#### Adding Random Slopes

You can also allow for **random slopes**, meaning that individuals can differ not only in their intercepts (baselines) but also in their rate of change over time.

```r

# Load necessary libraries
library(lme4)

# Fit a random slope model
model2 <- lmer(Outcome ~ Time + (Time | SubjectID), data = data)

# Display model summary to examine fixed and random effects
summary(model2)

```

Now, each child has their own growth rate!

#### Model Diagnostics and Visualization

Once weâ€™ve fitted our models, we need to assess how well they fit the data.

##### Model Fit: AIC, BIC, and Likelihood Ratio Tests

One way to assess model fit is by comparing models using **AIC** (Akaike Information Criterion) or **BIC** (Bayesian Information Criterion). A lower AIC or BIC indicates a better model fit.

```r
# Comparing model fit
AIC(model1, model2)
```

#### Residual Diagnostics

Check the residuals to ensure that the assumptions of normality and homoscedasticity hold.

```r
# Plot residuals
plot(model2)
```

If the residuals are roughly normally distributed and homoscedastic, the model is well-fitted.

#### Visualization

Visualizing both the raw data and the fitted model is key for understanding your results.

##### Plotting Raw Data

Use `ggplot2` to visualize the data points and individual growth trajectories.

```r
# Load ggplot2 library
library(ggplot2)

# Plot raw data
ggplot(data, aes(x = Time, y = Outcome, color = factor(SubjectID))) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE) +
    labs(
        title = "Raw Data: Individual Growth Trajectories",
        x = "Time (years)",
        y = "Outcome",
        color = "Subject ID"
    )

```

##### Plotting Predicted Values

Next, plot the predicted growth curves for each individual based on the model:

```r
# Add predicted values to the dataset
data$pred <- predict(model2)

# Plot predicted values
ggplot(data, aes(x = Time, y = pred, color = factor(SubjectID))) +
    geom_line() +
    labs(
        title = "Predicted Growth Curves by Subject",
        x = "Time (years)",
        y = "Predicted Outcome",
        color = "Subject ID"
    )

```

### ðŸ› Playground Exercises

We encourage you to explore real-world examples and experiment with models in our interactive sandbox. You can practice fitting models like random intercept models, random slope models, and compare their AIC values through the **[Playground](www.playground.com)** section of the website.

There, you will find exercises like:

- **Exercise 1**: Fit a random intercept model using a dataset and interpret the fixed and random effects.
- **Exercise 2**: Compare a random intercept model vs. a random slope model, and analyze their AIC values to determine which model fits better.

Head over to **[www.playground.com](www.playground.com)** to get started!

### Advanced Topics (Optional)

#### Nested Models and Cross-Classified Models

In more complex cases, you might have individuals nested within larger groups (e.g., students within schools). LMMs can handle this by including random effects at multiple levels, accounting for both within-group and between-group variability.

#### Time-Varying Covariates

LMMs can incorporate time-varying covariatesâ€”variables that change over time for each individual, such as stress levels or income. This allows you to model how changes in covariates over time influence the outcome.

#### Random Slopes with Interaction Terms

Incorporating interaction terms in random slope models allows you to examine how the relationship between predictors (e.g., time) and the outcome varies across individuals or groups. This is useful when you expect the effect of a predictor to differ depending on group membership or another variable.

#### Model Selection and Penalization

When fitting complex models, itâ€™s important to choose the right balance of fixed and random effects. Techniques like AIC and BIC can help with model selection. Additionally, penalization methods like Lasso or Ridge regularization may be considered to prevent overfitting, especially in high-dimensional data.

#### Generalized Linear Mixed Models (GLMMs)

For non-continuous outcomes (e.g., binary, count), generalized linear mixed models (GLMMs) extend LMMs by incorporating different link functions and distributions. For example, the `glmer()` function in `lme4` can be used for binary logistic regression or Poisson regression in the mixed-effects framework.

#### Bayesian Linear Mixed Models

Bayesian methods for LMMs provide an alternative to frequentist approaches, allowing for more flexible modeling with prior distributions on the fixed and random effects. Packages like `brms` and `rstanarm` make it easier to fit Bayesian linear mixed models.

### Conclusion

Congratulations on completing this tutorial! Youâ€™ve learned how to fit and interpret linear mixed-effects models for longitudinal data, assess model fit, and visualize the results.

For more advanced topics, continue exploring nested models, GLMMs, and more complex structures as you build confidence with LMMs. Keep practicing, and donâ€™t hesitate to refer back to this guide as you progress.

Happy modeling!

---

### Further Resources for Learning About Linear Mixed Models

If you're looking to deepen your understanding of Linear Mixed Models (LMMs) and explore more advanced topics, here are some excellent resources:

- **Books**:
  - *"Linear Mixed-Effects Models using R"* by Douglas Bates: A comprehensive guide on LMMs using the `lme4` package in R.
  - *"Applied Longitudinal Analysis"* by Garrett Fitzmaurice: An excellent resource for learning about mixed models in the context of longitudinal data.
  
- **Online Tutorials**:
  - [RStudio LMM Documentation](https://lme4.r-forge.r-project.org/): Documentation for the `lme4` package, which is widely used for fitting linear mixed models in R.
  - [Mixed Models in R by UCLA](https://stats.oarc.ucla.edu/r/dae/mixed-effects-models/): A beginner-friendly online tutorial that provides step-by-step guidance on how to fit mixed models in R.

- **Courses**:
  - [Coursera's "Linear Mixed-Effects Models" Course](https://www.coursera.org/learn/linear-mixed-models): A course that covers mixed-effects models with practical applications in R.
  
- **Video Tutorials**:
  - [YouTube: Linear Mixed Models Tutorial](https://www.youtube.com/watch?v=Qcojb2HhxJY): A comprehensive video tutorial that explains LMMs and how to apply them in R.

These resources should help you expand your knowledge and gain more practical experience with Linear Mixed Models.


